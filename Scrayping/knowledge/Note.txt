著作権
：Webページは基本的に著作物

注意する権利
複製権：収集したWebページを保存する権利
翻案権：収集したWebページから新たな著作物を創造する権利
公衆送信権：収集したWebページをサーバーから公開する権利

：私的使用の範囲内の複製など、使用目的によっては著作権者の許諾なく自由に行える

2009年の著作権法改正
：情報解析、検索エンジンサービスの提供が目的ならOK

一定の条件に注意
：会員のみが閲覧可能なサイトのクロールには著作権者の許諾が必要なこと
：robots.txtやrobots metaタグで拒否されているページをクロールしないこと
：検索結果では元のWebページにリンクすること
：検索結果として表示する著作物は必要と認められる限度内であること
：違法コンテンツであることを知った場合は公衆送信をやめること


利用規約と個人情報
：明示的に禁止な場合もある
：個人情報の収集は特に注意。EUのデータなど。


クロール先の負担
：クロール間隔は１秒以上
：robots.txtのCrawl delayディレクティブで指示可能
：負荷は以下の順に大きくなる「XML < HTML < Java, PHP」


robots.txtによるクローラーへの指示
：Webサイトの管理者がクローラーに対して特定のページをクロールしないよう指示
：Webサイトのトップディレクトリに配置される
：robots.txtの中身はRobotsExclusionProtocolとして標準化される
：robots.txtが存在しない場合は、すべてのページのクロールが許可されている
：RobotsExclusionProtocolは厳密な仕様ではなく、実装によって解釈が異なる
：HTTP401または403のエラーが返ってきた場合の解釈も異なる

robots.txtやrobots metaタグ
：拘束力のない紳士協定
：指示に従うかどうかは、クローラー作成者が決められる

User-agent
：「Disallow:/」は、すべてのページを許可しない

robots metaタグ
：HTMLのmetaタグにクローラーへの指示が記述
：<meta name="robots"content="no index">
：no follow = このページ内のリンクをたどることを許可しない
：no archive = このページをアーカイブとして保存することを許可しない
：no index = このページを検索エンジンにインデックスすることを許可しない


XMLサイトマップ
：Webサイトの管理者がクローラーに対してクロールして欲しいURLのリストを提示
：無駄に探索せず、効率的
：ルート要素としてurlset要素があり、複数のurl要素を持つ
：loc要素にページの絶対URLが記述
：1つのXMLサイトマップは10MB以内、かつ含まれるURLが50,000個以内
：この制限を超える場合は複数のサイトマップに分割


連絡先の明示
：Webサイトの管理者がクローラーからのアクセスに困っているときに、
：クローラーの作成者に連絡する手段があると問題を解決しやすくなる
：クローラーが送信するHTTPリクエストのUserAgentに連絡先のURLやメールアドレスを書く


ステータスコードとエラー処理
：Webサーバーにアクセスする際のエラーは、大きく2種類
：ネットワークレベルのエラー
：HTTPレベルのエラー
：4xxがクライアントのエラー
：5xxがサーバーのエラー
：対処法は、時間を置いてリトライするか、そのページを単に諦める
：リトライ回数が増える度に指数関数的に間隔を増やす（1秒、2秒、4秒...）

ネットワークレベルのエラー
：DNS解決の失敗や通信のタイムアウトなど
：サーバーと正常に通信できていない場合に発生
：設定が間違っているのでない限り一時的なエラーと考えられま

HTTPレベルのエラー
：サーバーと正常に通信できているものの、HTTPレベルで問題がある場合に発生
：WebサーバーはHTTPレスポンスのステータスコードでリクエストの結果を返す


繰り返しの実行を前提とした設計
：更新されたデータだけを取得できるようにする
：エラーなどで停止した後に途中から再開できるようにする
：更新されたデータのみを取得する方法
：クロール先のWebサイトに変化があったときに検知する方法

更新されたデータだけを取得
：HTTPのキャッシュポリシーに従う

強いキャッシュ
：CacheControlとExpiresが該当
：有効期限が切れるまではリクエストを送らず、キャッシュされたレスポンスを使う。
：キャッシュが有効な間はサーバーにリクエストを送らないため、サーバーに負荷を与えない。

弱いキャッシュ
：LastModifiedとETagが該当
：一度レスポンスをキャッシュすると、次回から条件付きのリクエストを送り、
　サーバーは更新がない場合に304（Not Modified）で空のレスポンスボディを返す
：304が返ってきた場合、クライアントはキャッシュされたレスポンスを使う
：サーバーには毎回リクエストを送るものの、レスポンスボディの処理を省略できるので、
　キャッシュがない場合よりはサーバー負荷を軽減できます。

プロキシサーバー
：クライアントとサーバーの間に立つ
：クライアントの代わりにサーバーと通信してレスポンスを返すためのサーバー
：プロキシサーバーが透過的にキャッシュすることで、Pythonのプログラムがシンプルになる
：複数のホストでキャッシュを共有できる
：しかし、実行時の依存性は増える
：RequestsでWebページを取得する場合、環境変数http_proxyとhttps_proxyに
　プロキシサーバーのURLを指定すると、プロキシ経由でWebページを取得できる
：環境変数のデフォルトは、OSのプロキシ設定


クロール先の変化を検知
：クローラー側で変化を検知し通知することで、プログラムを修正
：バリデーション（正規表現，library: JSON Schema）で対処

変化例
：CSSセレクターやXPathで取得しようとした要素が存在しなくなった
：＊要素は変わらず存在するものの、意図したのとは違う値が得られてしまった
